{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 Group 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chongyu He (ch3379)\n",
    "* Daniel Lee (dl3250)\n",
    "* Yiwen Ma (ym2775)\n",
    "* Runzi Qiang (rq2156)\n",
    "* Yifan Yang (yy2955)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As the data analysis became a key part of modern online services, number of recommendation methods have been developed. Content filtering is one recommender systems where we create a profile for each user to product to characterize its nature. For example, a movie profile could include attributes regarding its genre, participating actors, its box office popularity, etc. User profiles might include demographic information or answers provided on a suitable questionnaire. The profiles allow programs to associate users with matching products. \n",
    "\n",
    "Our group’s goal is to see the difference between the two models:\n",
    "SGD algorithm with temporal regularization and postprocessing SVD with KNN\n",
    "SGD algorithm with temporal regularization and postprocessing SVD with kernel ridge regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models:\n",
    "\n",
    "1. Stochastic Gradient Descent + Temporal Dynamics + KNN Postprocessing\n",
    "\n",
    "2. Stochastic Gradient Descent + Temporal Dynamics + Kernel Ridge Regression Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: Temporal Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Temporal Dynamics\n",
    "$$\n",
    "\\hat r = q_i^T p_u + b_{ui} \\\\\n",
    "b_{ui} = \\mu + b_i + b_u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Temporal Dynamics\n",
    "\n",
    "1. Time-Changing Item Bias\n",
    "    $$\n",
    "    b_i(t) = b_i + b_{i,Bin(t)}\n",
    "    $$\n",
    "    \n",
    "2. Time-Changing User Bias\n",
    "    \n",
    "    $$\n",
    "    dev_u(t) = sign(t − t_u) * |t − t_u|^β \\\\ \\\\\n",
    "    b_u(t) = b_u + \\alpha_u \\times dev_u(t)\n",
    "    $$\n",
    "    where\n",
    "    * $t_u$: mean date of rating by the user \n",
    "    * $\\beta$: a hyperaparameter\n",
    "    * $b_u$: stationary portion of the user bias\n",
    "\n",
    "     \n",
    "3. Time-Changing User Preference\n",
    "\n",
    "    $$\n",
    "    p_{uk}(t) = p_{uk} + \\alpha_{uk} \\times dev_u(t)\n",
    "    $$\n",
    "    \n",
    "    where\n",
    "    * $p_u$: stationary portion\n",
    "    * $\\alpha_u \\times dev_u(t)$: the portion that changes linearly over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put Them All Together\n",
    "\n",
    "$$\n",
    "\\hat r = \\mu + b_i(t) + b_u(t) + q_i^T p_u(t) \\\\\n",
    "error = r - \\hat r\n",
    "$$\n",
    "\n",
    "Objective:\n",
    "$$\n",
    "\\min \\sum_{u,i,t} (r_{u,i}(t) - \\mu - b_u - \\alpha_u dev_u(t) - b_i - b_{i, Bin(t)} - q_i^T p_u(t) )^2 \\\\\n",
    "+\\lambda (b_u^2 + \\alpha_u^2 + b_i^2 + b_{i,Bin(t}^2 + q_i^2 + p_{uk}^2+ \\alpha_{uk}^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "#### 1. KNN\n",
    "Item Similartiy:\n",
    "    $$s(q_i,q_j) =   \\frac{q_i^Tq_j}{||q_i|| ||q_j||}$$\n",
    "    \n",
    "    \n",
    "#### 2. Kernel Ridge Regression\n",
    "\n",
    "$$\\hat y^i =K(x_i^T ,X)(K(X,X)+λI)^{-1}y$$\n",
    "\n",
    "where\n",
    "* y: movies rated by a user\n",
    "* X: ith row of X is a normalized vector of features of one movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mfsgd(object):\n",
    "    def __init__(self, filename, test_size=0.1, random_state=0, n=10, penalty=0.5, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        param learning_rate: minimum 1e-6 \n",
    "        \"\"\"\n",
    "        self.data = mfsgd.preprocess(filename)\n",
    "        self.lr = max(learning_rate, 1e-6)\n",
    "        self.origlr = max(learning_rate, 1e-6)\n",
    "        self.decrement = 1\n",
    "        self.nepoch = 1e6\n",
    "        self.n = n\n",
    "        self.penalty = penalty\n",
    "        self.train_size = None\n",
    "        self.validation_size = 0\n",
    "        if test_size >= 1:\n",
    "            raise Exception('test_size must be < 1')\n",
    "        self.test_size = test_size\n",
    "        self.test = self.data.groupby('userId').apply(lambda x: x.sample(frac=test_size, random_state=random_state)).reset_index(level=0, drop=True)\n",
    "        self.n_users = len(self.data.loc[:, 'userId'].unique())\n",
    "        unique_items = self.data.loc[:, 'movieId'].unique()\n",
    "        self.n_items = len(unique_items)\n",
    "        self.item_mapping = dict(zip(unique_items, list(range(len(unique_items)))))\n",
    "        self.time_window = (self.data.loc[:, 'timestamp'].min(), self.data.loc[:, 'timestamp'].max()+1)\n",
    "        \n",
    "    def setLearningRateSchedule(self, start=0.01, decrement=0.1, nepoch=100):\n",
    "        \"\"\"\n",
    "        param start: starting learning rate\n",
    "        param decrement: multiplier to the learning rate per nepoch epochs\n",
    "        param nepoch: number of epochs between two decrements\n",
    "        \"\"\"\n",
    "        self.lr = start\n",
    "        self.origlr = start\n",
    "        self.decrement = decrement\n",
    "        self.nepoch = nepoch\n",
    "        return self\n",
    "    \n",
    "    def fit(self, train_size=0.7, user_nbins=10, item_nbins=3, beta=0.4, n_init=1, n_iter=50):\n",
    "        if train_size + self.test_size > 1:\n",
    "            train_size = 1 - self.test_size\n",
    "            warnings.warn('train size truncated to',train_size)\n",
    "        pct = train_size / (1 - self.test_size)\n",
    "        self.r = self.data.drop(self.test.index).groupby('userId').apply(lambda x: x.sample(frac=pct)).reset_index(level=0, drop=True)\n",
    "        self.train_size = train_size\n",
    "        self.beta = beta\n",
    "        self.user_nbins = user_nbins\n",
    "        self.user_binsize = self.__binify(self.time_window, self.user_nbins)\n",
    "        self.avg_user_bin = {k: self.__timestampToBin(v, self.user_binsize) for k, v in self.r.groupby('userId')['timestamp'].mean().items()}\n",
    "        self.item_nbins = item_nbins\n",
    "        self.item_binsize = self.__binify(self.time_window, self.item_nbins)\n",
    "        self.user_dict = self.r.groupby('userId')['movieId']\n",
    "        self.ru = self.user_dict.count().apply(lambda x:x**(-0.5))\n",
    "        self.train_loss = np.nan\n",
    "        for i in range(n_init):\n",
    "            result = self.__trainEach(n_iter)\n",
    "            if np.isnan(self.train_loss) or result['loss'] < self.train_loss:\n",
    "                self.mu = result['mu']\n",
    "                self.q = result['q']\n",
    "                self.p_user = result['p_user']\n",
    "                self.pa_user = result['pa_user']\n",
    "                self.b_user = result['b_user']\n",
    "                self.a_user = result['a_user']\n",
    "                self.b_item = result['b_item']\n",
    "                self.b_item_bin = result['b_item_bin']\n",
    "                self.y = result['y']\n",
    "                self.train_loss = result['loss']\n",
    "        self.__resetLR()\n",
    "        return self\n",
    "    \n",
    "    def validate(self):\n",
    "        if self.train_size + self.test_size == 1:\n",
    "            warnings.warn('no data can be used to validate')\n",
    "            return\n",
    "        self.validation_size = 1 - self.train_size - self.train_size\n",
    "        self.validation = self.data.drop(self.test.index.union(self.r.index)).groupby('userId').reset_index(level=0, drop=True)\n",
    "        rmse, r_pred = self.__computeLoss(dataset='validation')\n",
    "        print('validation rmse:', rmse)\n",
    "        return r_pred\n",
    "    \n",
    "    def predict(self, method='RSVD', **kwargs):\n",
    "        if self.train_size is None:\n",
    "            raise Exception('model is not trained')\n",
    "        if method == 'RSVD':\n",
    "            rmse, r_pred = self.__computeLoss(dataset='test')\n",
    "        else:\n",
    "            rmse, r_pred = self.__computeDefLoss(method, **kwargs)\n",
    "        print(method, 'test rmse:', rmse)\n",
    "        return r_pred\n",
    "        \n",
    "    def __trainEach(self, n_iter):\n",
    "        mu = np.random.uniform(-0.01, 0.01, 1)\n",
    "        q = np.random.uniform(-0.01, 0.01, (self.n, self.n_items))\n",
    "        p_user = np.random.uniform(-0.01, 0.01, (self.n, self.n_users))\n",
    "        pa_user = np.random.uniform(-0.01, 0.01, (self.n, self.n_users))\n",
    "        b_user = np.random.uniform(-0.01, 0.01, self.n_users)\n",
    "        a_user = np.random.uniform(-0.01, 0.01, self.n_users)\n",
    "        b_item = np.random.uniform(-0.01, 0.01, self.n_items)\n",
    "        b_item_bin = np.random.uniform(-0.01, 0.01, (self.item_nbins, self.n_items))\n",
    "        y = np.random.uniform(-0.01, 0.01, (self.n, self.n_items))\n",
    "        \n",
    "        c = 0\n",
    "        for it in range(n_iter):\n",
    "            loss = 0\n",
    "            sTime = time.time()\n",
    "            for ind, s in self.r.iterrows():\n",
    "                u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "                pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "                i_bin = self.__timestampToBin(t, self.item_binsize)\n",
    "                bi, bibin = b_item[i], b_item_bin[i_bin, i]\n",
    "                bu, au = b_user[u], a_user[u]\n",
    "                dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "                ru = self.ru[u+1]\n",
    "                user_items = [self.item_mapping[x] for x in self.user_dict.get_group(u+1)]\n",
    "                yu = np.sum(y[:, user_items], axis=1)\n",
    "                r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev+ru*yu)\n",
    "                res = r - r_hat\n",
    "                # update based on gradient\n",
    "                mu -= self.lr * self.__muDeriv(res)\n",
    "                q[:,i] -= self.lr * self.__qDeriv(res, pu, pua, qi, ru, yu, dev)\n",
    "                p_user[:,u] -= self.lr * self.__puDeriv(res, pu, qi)\n",
    "                pa_user[:, u] -= self.lr * self.__puaDeriv(res, pua, qi, dev)\n",
    "                b_user[u] -= self.lr * self.__buDeriv(res, bu)\n",
    "                a_user[u] -= self.lr *self.__auDeriv(res, au, dev)\n",
    "                b_item[i] -= self.lr * self.__biDeriv(res, bi)\n",
    "                b_item_bin[i_bin, i] -= self.lr * self.__bibinDeriv(res, bibin)\n",
    "                y[:, user_items] -= self.lr * self.__yuDeriv(res, qi, ru, y[:, user_items])\n",
    "                \n",
    "                loss += res**2\n",
    "            # update learning rate\n",
    "            c += 1\n",
    "            if not c%self.nepoch:\n",
    "                self.lr = max(self.lr * self.decrement, 1e-6)\n",
    "            \n",
    "            # use avg residual as loss\n",
    "            loss = np.sqrt(loss / len(self.r))\n",
    "            execTime = time.time() - sTime\n",
    "            \n",
    "            print('epoch', it+1, '----learning rate: {:.6f}'.format(self.lr), '----unpenalized training loss:', loss, \n",
    "                 '----execution time: %s'%execTime)\n",
    "        \n",
    "        return {'loss':loss,\n",
    "                'mu':mu,\n",
    "                'q':q,\n",
    "                'p_user':p_user,\n",
    "                'pa_user':pa_user,\n",
    "                'b_user':b_user,\n",
    "                'a_user':a_user,\n",
    "                'b_item':b_item,\n",
    "                'b_item_bin':b_item_bin,\n",
    "                'y':y}\n",
    "        \n",
    "    def __computeLoss(self, dataset='train', **kwargs):\n",
    "        loss = 0\n",
    "        r_pred = None\n",
    "        if dataset == 'train':\n",
    "            data = self.r\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin, y = kwargs['mu'], kwargs['q'], kwargs['p_user'], kwargs['pa_user'], kwargs['b_user'], kwargs['a_user'], kwargs['b_item'], kwargs['b_item_bin'], kwargs['y']\n",
    "        elif dataset in ['test', 'validation']:\n",
    "            data = self.test if dataset == 'test' else self.validation\n",
    "            r_pred = np.zeros(len(data))\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin, y = self.mu, self.q, self.p_user, self.pa_user, self.b_user, self.a_user, self.b_item, self.b_item_bin, self.y\n",
    "        else:\n",
    "            raise Exception('ambiguous compute loss inputs')\n",
    "        \n",
    "        for ind, s in data.reset_index().iterrows():\n",
    "            u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "            pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "            bi, bibin = b_item[i], b_item_bin[self.__timestampToBin(t, self.item_binsize), i]\n",
    "            bu, au = b_user[u], a_user[u]\n",
    "            dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "            ru = self.ru[u+1]\n",
    "            user_items = [self.item_mapping[x] for x in self.user_dict.get_group(u+1)]\n",
    "            yu = np.sum(y[:, user_items], axis=1)\n",
    "            r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev+ru*yu)\n",
    "            res = (r-r_hat)**2\n",
    "            if dataset == 'train':\n",
    "                loss += res + self.penalty*(bi**2+bibin**2+bu**2+au**2+npla.norm(pu)**2+npla.norm(pua)**2+npla.norm(qi)**2)\n",
    "            else:\n",
    "                loss += res\n",
    "                r_pred[ind] = r_hat\n",
    "\n",
    "        return np.sqrt(loss / len(data)), r_pred\n",
    "    \n",
    "    def __computeDefLoss(self, method='KNN', **kwargs):\n",
    "        gb = self.test.groupby('userId')\n",
    "        sum_res = 0\n",
    "        all_r = []\n",
    "        all_r_pred = []\n",
    "        for user in gb.groups.keys():\n",
    "            item_ind = sorted([self.item_mapping[x] for x in self.user_dict.get_group(user)])\n",
    "            X = normalize(np.transpose(self.q[:, item_ind]))\n",
    "            y = np.array(self.r.groupby('userId').get_group(user).sort_values(by='movieId')['rating'])\n",
    "            test_item_ind = [self.item_mapping[x] for x in gb.get_group(user)['movieId']]\n",
    "            test_X = normalize(np.transpose(self.q[:, test_item_ind]))\n",
    "            r = gb.get_group(user)['rating']\n",
    "            all_r.append(r)\n",
    "            if method == 'KNN':\n",
    "                y = [str(x) for x in y]\n",
    "                r_pred = KNeighborsClassifier(**kwargs).fit(X, y).predict(test_X)\n",
    "                r_pred = np.array([float(x) for x in r_pred])\n",
    "            elif method == 'KernelRidge':\n",
    "                r_pred = KernelRidge(**kwargs).fit(X, y).predict(test_X)\n",
    "            else:\n",
    "                raise Exception('NYI')\n",
    "            all_r_pred.append(r_pred)\n",
    "        all_r = np.concatenate(all_r)\n",
    "        all_r_pred = np.concatenate(all_r_pred)\n",
    "        rmse = np.sqrt(np.sum((all_r - all_r_pred)**2) / len(self.test))\n",
    "        \n",
    "        return rmse, all_r_pred\n",
    "    \n",
    "    # FIXME, update qDeriv\n",
    "    def __muDeriv(self, res):\n",
    "        return -res\n",
    "    \n",
    "    def __qDeriv(self, res, pu, pua, qi, ru, yu, dev):\n",
    "        return -res * (pu+pua*dev+ru*yu) + self.penalty * qi\n",
    "    \n",
    "    def __puDeriv(self, res, pu, qi):\n",
    "        return -res * qi + self.penalty * pu\n",
    "    \n",
    "    def __puaDeriv(self, res, pua, qi, dev):\n",
    "        return -res * qi * dev + self.penalty * pua\n",
    "    \n",
    "    def __buDeriv(self, res, bu):\n",
    "        return -res + self.penalty * bu\n",
    "    \n",
    "    def __auDeriv(self, res, au, dev):\n",
    "        return -res * dev + self.penalty * au\n",
    "    \n",
    "    def __biDeriv(self, res, bi):\n",
    "        return -res + self.penalty * bi\n",
    "    \n",
    "    def __bibinDeriv(self, res, bibin):\n",
    "        return -res + self.penalty * bibin\n",
    "    \n",
    "    def __yuDeriv(self, res, qi, ru, yu):\n",
    "        return -res * qi[:, np.newaxis] * ru + self.penalty * yu\n",
    "    \n",
    "    # FIXME, add y and R(u)^(-1/2)\n",
    "    def __dev(self, t, avg, b):\n",
    "        return np.sign(t-avg) * np.abs(t-avg)**b\n",
    "    \n",
    "    def __binify(self, window, nbins):\n",
    "        return (window[1] - window[0]) / nbins\n",
    "    \n",
    "    def __timestampToBin(self, t, binsize):\n",
    "        if t < self.time_window[0] or t > self.time_window[1]:\n",
    "            raise Exception('t outside of time window')\n",
    "        return int((t - self.time_window[0]) // binsize)\n",
    "    \n",
    "    def __resetLR(self):\n",
    "        self.lr = self.origlr\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('G:\\mawenwen\\Columbia\\Fall 2019\\Applied Data Science\\proj4','fall2019-project4-sec1-grp4-master\\data\\ml-latest-small','ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x202f4a3c710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mfsgd(filename=f, test_size=0.1, n=30, penalty=0.1) # learning rate should not be > 0.1 as it results in overflow in loss calculation\n",
    "s.setLearningRateSchedule(start=0.05, decrement=0.2, nepoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91545374] ----execution time: 74.87599110603333\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.86125939] ----execution time: 75.81782841682434\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.8280277] ----execution time: 82.8188533782959\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.79321738] ----execution time: 99.7977340221405\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.75668924] ----execution time: 90.78104066848755\n",
      "epoch 6 ----learning rate: 0.010000 ----unpenalized training loss: [0.70522007] ----execution time: 75.35979270935059\n",
      "epoch 7 ----learning rate: 0.010000 ----unpenalized training loss: [0.69489607] ----execution time: 76.86985754966736\n",
      "epoch 8 ----learning rate: 0.010000 ----unpenalized training loss: [0.68831764] ----execution time: 74.90982174873352\n",
      "epoch 9 ----learning rate: 0.010000 ----unpenalized training loss: [0.68274608] ----execution time: 74.87777900695801\n",
      "epoch 10 ----learning rate: 0.002000 ----unpenalized training loss: [0.67762191] ----execution time: 74.65812110900879\n",
      "epoch 11 ----learning rate: 0.002000 ----unpenalized training loss: [0.67556884] ----execution time: 75.55739617347717\n",
      "epoch 12 ----learning rate: 0.002000 ----unpenalized training loss: [0.67228036] ----execution time: 74.9938645362854\n",
      "epoch 13 ----learning rate: 0.002000 ----unpenalized training loss: [0.67037078] ----execution time: 74.44936466217041\n",
      "epoch 14 ----learning rate: 0.002000 ----unpenalized training loss: [0.668883] ----execution time: 74.88188767433167\n",
      "epoch 15 ----learning rate: 0.000400 ----unpenalized training loss: [0.66759262] ----execution time: 75.72918128967285\n",
      "epoch 16 ----learning rate: 0.000400 ----unpenalized training loss: [0.67035985] ----execution time: 76.40582227706909\n",
      "epoch 17 ----learning rate: 0.000400 ----unpenalized training loss: [0.66927267] ----execution time: 76.19786334037781\n",
      "epoch 18 ----learning rate: 0.000400 ----unpenalized training loss: [0.66841737] ----execution time: 76.11367201805115\n",
      "epoch 19 ----learning rate: 0.000400 ----unpenalized training loss: [0.66772831] ----execution time: 77.4092960357666\n",
      "epoch 20 ----learning rate: 0.000080 ----unpenalized training loss: [0.66714392] ----execution time: 77.21005630493164\n",
      "epoch 21 ----learning rate: 0.000080 ----unpenalized training loss: [0.66920818] ----execution time: 76.80108308792114\n",
      "epoch 22 ----learning rate: 0.000080 ----unpenalized training loss: [0.66851072] ----execution time: 76.78957319259644\n",
      "epoch 23 ----learning rate: 0.000080 ----unpenalized training loss: [0.66825981] ----execution time: 76.83793902397156\n",
      "epoch 24 ----learning rate: 0.000080 ----unpenalized training loss: [0.66802671] ----execution time: 77.15731358528137\n",
      "epoch 25 ----learning rate: 0.000016 ----unpenalized training loss: [0.66780908] ----execution time: 76.28913354873657\n",
      "epoch 26 ----learning rate: 0.000016 ----unpenalized training loss: [0.66821328] ----execution time: 76.4177017211914\n",
      "epoch 27 ----learning rate: 0.000016 ----unpenalized training loss: [0.66798625] ----execution time: 76.32525038719177\n",
      "epoch 28 ----learning rate: 0.000016 ----unpenalized training loss: [0.66791368] ----execution time: 76.12155747413635\n",
      "epoch 29 ----learning rate: 0.000016 ----unpenalized training loss: [0.66786152] ----execution time: 80.2863883972168\n",
      "epoch 30 ----learning rate: 0.000003 ----unpenalized training loss: [0.66781372] ----execution time: 80.72739577293396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x202f4a3c710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.fit(train_size=0.9, user_nbins=10, item_nbins=3, beta=0.6, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_validate = s.validate() # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.85535767]\n"
     ]
    }
   ],
   "source": [
    "r_test_RSVD = s.predict(method='RSVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN test rmse: 1.321723622393826\n"
     ]
    }
   ],
   "source": [
    "r_test_KNN = s.predict(method='KNN', n_neighbors=1) # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelRidge test rmse: 0.9883413377798275\n"
     ]
    }
   ],
   "source": [
    "r_test_ridge = s.predict(method='KernelRidge', alpha=0.5, kernel='rbf', gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSVD gives the best out-of-sample RSVD of 0.855, while kernel ridge and KNN deliver 0.988 and 1.322 RMSE respectively. The outperformance Kernel ridge over KNN is expected as KNN is a classification method. Classification disallows the existence of ambiguous ratings (such as 3.25), so the expense of misclassification is expected to be bigger than kernel ridge regression. \n",
    "\n",
    "The outperformance of RSVD could be explained by two things:\n",
    "1. When performing matrix factorization, it is the loss function of RSVD that gets optimized, rather than that of KNN or kernel ridge;\n",
    "2. Temporal dynamics are incorporated in the loss function. The binified time components are able to explain some variation in the sample dataset, so it is possible that sometimes a rating is dominated by the bias in time dimension. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
