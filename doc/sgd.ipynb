{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 Group 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chongyu He (ch3379)\n",
    "* Daniel Lee (dl3250)\n",
    "* Yiwen Ma (ym2775)\n",
    "* Runzi Qiang (rq2156)\n",
    "* Yifan Yang (yy2955)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models:\n",
    "\n",
    "1. Stochastic Gradient Descent + Temporal Dynamics + KNN Postprocessing\n",
    "\n",
    "2. Stochastic Gradient Descent + Temporal Dynamics + Kernel Ridge Regression Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: Temporal Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Temporal Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat r = q_i^T p_u + b_{ui} \\\\\n",
    "b_{ui} = \\mu + b_i + b_u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Temporal Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time-Changing Item Bias\n",
    "    $$\n",
    "    bi(t) = b_i + b_{i,Bin(t)}\n",
    "    $$\n",
    "    \n",
    "2. Time-Changing User Bias\n",
    "    \n",
    "    $$\n",
    "    dev_u(t) = sign(t − t_u) * |t − t_u|^β \\\\ \\\\\n",
    "    b_u(t) = b_u + \\alpha_u \\times dev_u(t)\n",
    "    $$\n",
    "    where\n",
    "    * $t_u$: mean date of rating by the user \n",
    "    * $\\beta$: a hyperaparameter\n",
    "    * $b_u$: stationary portion of the user bias\n",
    "\n",
    "     \n",
    "3. Time-Changing User Preference\n",
    "\n",
    "    $$\n",
    "    p_t(t) + p_u + \\alpha_u \\times dev_u(t)\n",
    "    $$\n",
    "    \n",
    "    where\n",
    "    * $p_u$: stationary portion\n",
    "    * $\\alpha_u \\times dev_u(t)$: the portion that changes linearly over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put Them All Together\n",
    "\n",
    "$$\n",
    "\\hat r = \\mu + b_i(t) + b_u(t) + q_i^T p_u(t) \\\\\n",
    "error = r - \\hat r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. KNN\n",
    "Item Similartiy:\n",
    "    $$s(q_i,q_j) =   \\frac{q_i^Tq_j}{||q_i|| ||q_j||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat y^i =K(x_i^T ,X)(K(X,X)+λI)^{-1}y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mfsgd(object):\n",
    "    def __init__(self, filename, test_size=0.1, random_state=0, n=10, penalty=0.5, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        param learning_rate: minimum 1e-6 \n",
    "        \"\"\"\n",
    "        self.data = mfsgd.preprocess(filename)\n",
    "        self.lr = max(learning_rate, 1e-6)\n",
    "        self.origlr = max(learning_rate, 1e-6)\n",
    "        self.decrement = 1\n",
    "        self.nepoch = 1e6\n",
    "        self.n = n\n",
    "        self.penalty = penalty\n",
    "        self.train_size = None\n",
    "        self.validation_size = 0\n",
    "        if test_size >= 1:\n",
    "            raise Exception('test_size must be < 1')\n",
    "        self.test_size = test_size\n",
    "        self.test = self.data.groupby('userId').apply(lambda x: x.sample(frac=test_size, random_state=random_state)).reset_index(level=0, drop=True)\n",
    "        self.n_users = len(self.data.loc[:, 'userId'].unique())\n",
    "        unique_items = self.data.loc[:, 'movieId'].unique()\n",
    "        self.n_items = len(unique_items)\n",
    "        self.item_mapping = dict(zip(unique_items, list(range(len(unique_items)))))\n",
    "        self.time_window = (self.data.loc[:, 'timestamp'].min(), self.data.loc[:, 'timestamp'].max()+1)\n",
    "        \n",
    "    def setLearningRateSchedule(self, start=0.01, decrement=0.1, nepoch=100):\n",
    "        \"\"\"\n",
    "        param start: starting learning rate\n",
    "        param decrement: multiplier to the learning rate per nepoch epochs\n",
    "        param nepoch: number of epochs between two decrements\n",
    "        \"\"\"\n",
    "        self.lr = start\n",
    "        self.origlr = start\n",
    "        self.decrement = decrement\n",
    "        self.nepoch = nepoch\n",
    "        return self\n",
    "    \n",
    "    def fit(self, train_size=0.7, user_nbins=10, item_nbins=3, beta=0.4, n_init=1, n_iter=50, verbose = False):\n",
    "        if train_size + self.test_size > 1:\n",
    "            train_size = 1 - self.test_size\n",
    "            warnings.warn('train size truncated to',train_size)\n",
    "        pct = train_size / (1 - self.test_size)\n",
    "        self.r = self.data.drop(self.test.index).groupby('userId').apply(lambda x: x.sample(frac=pct)).reset_index(level=0, drop=True)\n",
    "        self.train_size = train_size\n",
    "        self.beta = beta\n",
    "        self.user_nbins = user_nbins\n",
    "        self.user_binsize = self.__binify(self.time_window, self.user_nbins)\n",
    "        self.avg_user_bin = {k: self.__timestampToBin(v, self.user_binsize) for k, v in self.r.groupby('userId')['timestamp'].mean().items()}\n",
    "        self.item_nbins = item_nbins\n",
    "        self.item_binsize = self.__binify(self.time_window, self.item_nbins)\n",
    "        self.user_dict = self.r.groupby('userId')['movieId']\n",
    "        self.ru = self.user_dict.count().apply(lambda x:x**(-0.5))\n",
    "        self.train_loss = np.nan\n",
    "        for i in range(n_init):\n",
    "            result = self.__trainEach(n_iter, verbose)\n",
    "            if np.isnan(self.train_loss) or result['loss'] < self.train_loss:\n",
    "                self.mu = result['mu']\n",
    "                self.q = result['q']\n",
    "                self.p_user = result['p_user']\n",
    "                self.pa_user = result['pa_user']\n",
    "                self.b_user = result['b_user']\n",
    "                self.a_user = result['a_user']\n",
    "                self.b_item = result['b_item']\n",
    "                self.b_item_bin = result['b_item_bin']\n",
    "                self.y = result['y']\n",
    "                self.train_loss = result['loss']\n",
    "        self.__resetLR()\n",
    "        return self\n",
    "    \n",
    "    def validate(self):\n",
    "        if self.train_size + self.test_size == 1:\n",
    "            warnings.warn('no data can be used to validate')\n",
    "            return\n",
    "        self.validation_size = 1 - self.train_size - self.train_size\n",
    "        self.validation = self.data.drop(self.test.index.union(self.r.index)).groupby('userId').reset_index(level=0, drop=True)\n",
    "        rmse, r_pred = self.__computeLoss(dataset='validation')\n",
    "        print('validation rmse:', rmse)\n",
    "        return r_pred\n",
    "    \n",
    "    def predict(self, method='RSVD', **kwargs):\n",
    "        if self.train_size is None:\n",
    "            raise Exception('model is not trained')\n",
    "        if method == 'RSVD':\n",
    "            rmse, r_pred = self.__computeLoss(dataset='test')\n",
    "        else:\n",
    "            rmse, r_pred = self.__computeDefLoss(method, **kwargs)\n",
    "        print(method, 'test rmse:', rmse)\n",
    "        return r_pred\n",
    "        \n",
    "    def __trainEach(self, n_iter, verbose = True):\n",
    "        mu = np.random.uniform(-0.01, 0.01, 1)\n",
    "        q = np.random.uniform(-0.01, 0.01, (self.n, self.n_items))\n",
    "        p_user = np.random.uniform(-0.01, 0.01, (self.n, self.n_users))\n",
    "        pa_user = np.random.uniform(-0.01, 0.01, (self.n, self.n_users))\n",
    "        b_user = np.random.uniform(-0.01, 0.01, self.n_users)\n",
    "        a_user = np.random.uniform(-0.01, 0.01, self.n_users)\n",
    "        b_item = np.random.uniform(-0.01, 0.01, self.n_items)\n",
    "        b_item_bin = np.random.uniform(-0.01, 0.01, (self.item_nbins, self.n_items))\n",
    "        y = np.random.uniform(-0.01, 0.01, (self.n, self.n_items))\n",
    "        \n",
    "        c = 0\n",
    "        for it in range(n_iter):\n",
    "            loss = 0\n",
    "            sTime = time.time()\n",
    "            for ind, s in self.r.iterrows():\n",
    "                u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "                pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "                i_bin = self.__timestampToBin(t, self.item_binsize)\n",
    "                bi, bibin = b_item[i], b_item_bin[i_bin, i]\n",
    "                bu, au = b_user[u], a_user[u]\n",
    "                dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "                ru = self.ru[u+1]\n",
    "                user_items = [self.item_mapping[x] for x in self.user_dict.get_group(u+1)]\n",
    "                yu = np.sum(y[:, user_items], axis=1)\n",
    "                r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev+ru*yu)\n",
    "                res = r - r_hat\n",
    "                # update based on gradient\n",
    "                mu -= self.lr * self.__muDeriv(res)\n",
    "                q[:,i] -= self.lr * self.__qDeriv(res, pu, pua, qi, ru, yu, dev)\n",
    "                p_user[:,u] -= self.lr * self.__puDeriv(res, pu, qi)\n",
    "                pa_user[:, u] -= self.lr * self.__puaDeriv(res, pua, qi, dev)\n",
    "                b_user[u] -= self.lr * self.__buDeriv(res, bu)\n",
    "                a_user[u] -= self.lr *self.__auDeriv(res, au, dev)\n",
    "                b_item[i] -= self.lr * self.__biDeriv(res, bi)\n",
    "                b_item_bin[i_bin, i] -= self.lr * self.__bibinDeriv(res, bibin)\n",
    "                y[:, user_items] -= self.lr * self.__yuDeriv(res, qi, ru, y[:, user_items])\n",
    "                \n",
    "                loss += res**2\n",
    "            # update learning rate\n",
    "            c += 1\n",
    "            if not c%self.nepoch:\n",
    "                self.lr = max(self.lr * self.decrement, 1e-6)\n",
    "            \n",
    "            # use avg residual as loss\n",
    "            loss = np.sqrt(loss / len(self.r))\n",
    "            execTime = time.time() - sTime\n",
    "            \n",
    "            print('epoch', it+1, '----learning rate: {:.6f}'.format(self.lr), '----unpenalized training loss:', loss, \n",
    "                 '----execution time: %.2f'%execTime)\n",
    "        \n",
    "        return {'loss':loss,\n",
    "                'mu':mu,\n",
    "                'q':q,\n",
    "                'p_user':p_user,\n",
    "                'pa_user':pa_user,\n",
    "                'b_user':b_user,\n",
    "                'a_user':a_user,\n",
    "                'b_item':b_item,\n",
    "                'b_item_bin':b_item_bin,\n",
    "                'y':y}\n",
    "        \n",
    "    def __computeLoss(self, dataset='train', **kwargs):\n",
    "        loss = 0\n",
    "        r_pred = None\n",
    "        if dataset == 'train':\n",
    "            data = self.r\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin, y = kwargs['mu'], kwargs['q'], kwargs['p_user'], kwargs['pa_user'], kwargs['b_user'], kwargs['a_user'], kwargs['b_item'], kwargs['b_item_bin'], kwargs['y']\n",
    "        elif dataset in ['test', 'validation']:\n",
    "            data = self.test if dataset == 'test' else self.validation\n",
    "            r_pred = np.zeros(len(data))\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin, y = self.mu, self.q, self.p_user, self.pa_user, self.b_user, self.a_user, self.b_item, self.b_item_bin, self.y\n",
    "        else:\n",
    "            raise Exception('ambiguous compute loss inputs')\n",
    "        \n",
    "        for ind, s in data.reset_index().iterrows():\n",
    "            u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "            pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "            bi, bibin = b_item[i], b_item_bin[self.__timestampToBin(t, self.item_binsize), i]\n",
    "            bu, au = b_user[u], a_user[u]\n",
    "            dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "            ru = self.ru[u+1]\n",
    "            user_items = [self.item_mapping[x] for x in self.user_dict.get_group(u+1)]\n",
    "            yu = np.sum(y[:, user_items], axis=1)\n",
    "            r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev+ru*yu)\n",
    "            res = (r-r_hat)**2\n",
    "            if dataset == 'train':\n",
    "                loss += res + self.penalty*(bi**2+bibin**2+bu**2+au**2+npla.norm(pu)**2+npla.norm(pua)**2+npla.norm(qi)**2)\n",
    "            else:\n",
    "                loss += res\n",
    "                r_pred[ind] = r_hat\n",
    "\n",
    "        return np.sqrt(loss / len(data)), r_pred\n",
    "    \n",
    "    def __computeDefLoss(self, method='KNN', **kwargs):\n",
    "        gb = self.test.groupby('userId')\n",
    "        sum_res = 0\n",
    "        all_r = []\n",
    "        all_r_pred = []\n",
    "        for user in gb.groups.keys():\n",
    "            item_ind = sorted([self.item_mapping[x] for x in self.user_dict.get_group(user)])\n",
    "            X = normalize(np.transpose(self.q[:, item_ind]))\n",
    "            y = np.array(self.r.groupby('userId').get_group(user).sort_values(by='movieId')['rating'])\n",
    "            test_item_ind = [self.item_mapping[x] for x in gb.get_group(user)['movieId']]\n",
    "            test_X = normalize(np.transpose(self.q[:, test_item_ind]))\n",
    "            r = gb.get_group(user)['rating']\n",
    "            all_r.append(r)\n",
    "            if method == 'KNN':\n",
    "                y = [str(x) for x in y]\n",
    "                r_pred = KNeighborsClassifier(**kwargs).fit(X, y).predict(test_X)\n",
    "                r_pred = np.array([float(x) for x in r_pred])\n",
    "            elif method == 'KernelRidge':\n",
    "                r_pred = KernelRidge(**kwargs).fit(X, y).predict(test_X)\n",
    "            else:\n",
    "                raise Exception('NYI')\n",
    "            all_r_pred.append(r_pred)\n",
    "        all_r = np.concatenate(all_r)\n",
    "        all_r_pred = np.concatenate(all_r_pred)\n",
    "        rmse = np.sqrt(np.sum((all_r - all_r_pred)**2) / len(self.test))\n",
    "        \n",
    "        return rmse, all_r_pred\n",
    "    \n",
    "    # FIXME, update qDeriv\n",
    "    def __muDeriv(self, res):\n",
    "        return -res\n",
    "    \n",
    "    def __qDeriv(self, res, pu, pua, qi, ru, yu, dev):\n",
    "        return -res * (pu+pua*dev+ru*yu) + self.penalty * qi\n",
    "    \n",
    "    def __puDeriv(self, res, pu, qi):\n",
    "        return -res * qi + self.penalty * pu\n",
    "    \n",
    "    def __puaDeriv(self, res, pua, qi, dev):\n",
    "        return -res * qi * dev + self.penalty * pua\n",
    "    \n",
    "    def __buDeriv(self, res, bu):\n",
    "        return -res + self.penalty * bu\n",
    "    \n",
    "    def __auDeriv(self, res, au, dev):\n",
    "        return -res * dev + self.penalty * au\n",
    "    \n",
    "    def __biDeriv(self, res, bi):\n",
    "        return -res + self.penalty * bi\n",
    "    \n",
    "    def __bibinDeriv(self, res, bibin):\n",
    "        return -res + self.penalty * bibin\n",
    "    \n",
    "    def __yuDeriv(self, res, qi, ru, yu):\n",
    "        return -res * qi[:, np.newaxis] * ru + self.penalty * yu\n",
    "    \n",
    "    # FIXME, add y and R(u)^(-1/2)\n",
    "    def __dev(self, t, avg, b):\n",
    "        return np.sign(t-avg) * np.abs(t-avg)**b\n",
    "    \n",
    "    def __binify(self, window, nbins):\n",
    "        return (window[1] - window[0]) / nbins\n",
    "    \n",
    "    def __timestampToBin(self, t, binsize):\n",
    "        if t < self.time_window[0] or t > self.time_window[1]:\n",
    "            raise Exception('t outside of time window')\n",
    "        return int((t - self.time_window[0]) // binsize)\n",
    "    \n",
    "    def __resetLR(self):\n",
    "        self.lr = self.origlr\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '../data/ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x1a1a35bc10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mfsgd(filename=f, test_size=0.1, n=30, penalty=0.1) # learning rate should not be > 0.1 as it results in overflow in loss calculation\n",
    "s.setLearningRateSchedule(start=0.05, decrement=0.2, nepoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91548826] ----execution time: 87.41\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.85692827] ----execution time: 86.44\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.82300284] ----execution time: 85.13\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.78813722] ----execution time: 91.35\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.75350661] ----execution time: 85.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyfyifan/miniconda3/envs/data/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: no data can be used to validate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.93878846]\n",
      "KNN test rmse: 1.3165599229485745\n",
      "KernelRidge test rmse: 0.982701926260962\n",
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91560107] ----execution time: 79.55\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.85718616] ----execution time: 73.29\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.82429635] ----execution time: 75.78\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.79018945] ----execution time: 72.64\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.7546556] ----execution time: 76.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyfyifan/miniconda3/envs/data/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: no data can be used to validate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.95889132]\n",
      "KNN test rmse: 1.3294315660887206\n",
      "KernelRidge test rmse: 0.9817333664174925\n",
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91563154] ----execution time: 81.44\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.84853106] ----execution time: 73.82\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.80862911] ----execution time: 76.46\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.77150066] ----execution time: 77.51\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.73598216] ----execution time: 74.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyfyifan/miniconda3/envs/data/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: no data can be used to validate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.96715757]\n",
      "KNN test rmse: 1.3158165635720558\n",
      "KernelRidge test rmse: 0.9787010267908879\n",
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91558674] ----execution time: 71.65\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.84756813] ----execution time: 71.27\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.80626472] ----execution time: 71.25\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.76881702] ----execution time: 73.46\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.73382263] ----execution time: 75.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyfyifan/miniconda3/envs/data/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: no data can be used to validate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.93847227]\n",
      "KNN test rmse: 1.3419767564979208\n",
      "KernelRidge test rmse: 0.9884662640967907\n",
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91590607] ----execution time: 85.66\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.84200003] ----execution time: 81.64\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.79525327] ----execution time: 84.44\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.75380986] ----execution time: 81.25\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.71763434] ----execution time: 81.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyfyifan/miniconda3/envs/data/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: no data can be used to validate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.95187696]\n",
      "KNN test rmse: 1.3259054883790709\n",
      "KernelRidge test rmse: 0.9820373520922189\n",
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91545295] ----execution time: 82.22\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.84186161] ----execution time: 84.73\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.79496503] ----execution time: 77.78\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.75374221] ----execution time: 82.57\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.71704297] ----execution time: 84.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyfyifan/miniconda3/envs/data/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: no data can be used to validate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSVD test rmse: [0.93492348]\n",
      "KNN test rmse: 1.3101381241266519\n",
      "KernelRidge test rmse: 0.9795465257665353\n",
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91532002] ----execution time: 79.15\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.85578301] ----execution time: 78.64\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.82108297] ----execution time: 81.41\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.78651057] ----execution time: 72.84\n"
     ]
    }
   ],
   "source": [
    "user_bins = [5,10,20]\n",
    "item_bins = [5,10,20]\n",
    "betas = [0.1, 0.6]\n",
    "\n",
    "validate_result = {}\n",
    "RSVD_test_result = {}\n",
    "KNN_test_result = {}\n",
    "ridge_test_result = {}\n",
    "\n",
    "for ub in user_bins:\n",
    "    for ib in item_bins:\n",
    "        for beta in betas:\n",
    "            s.fit(train_size=0.9, user_nbins= ub, item_nbins=ib, beta= beta, n_iter=5, verbose= True)\n",
    "            key = f\"ub{ub}ib{ib}beta{beta}\"\n",
    "            validate_result[key] = s.validate()\n",
    "            RSVD_test_result[key] = s.predict(method='RSVD')\n",
    "            KNN_test_result[key] =  s.predict(method='KNN', n_neighbors=1)\n",
    "            ridge_test_result[key] = s.predict(method='KernelRidge', alpha=0.5, kernel='rbf', gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_validate = s.validate() # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test_RSVD = s.predict(method='RSVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test_KNN = s.predict(method='KNN', n_neighbors=1) # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test_ridge = s.predict(method='KernelRidge', alpha=0.5, kernel='rbf', gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSVD gives the best out-of-sample RSVD of 0.855, while kernel ridge and KNN deliver 0.988 and 1.322 RMSE respectively. The outperformance Kernel ridge over KNN is expected as KNN is a classification method. Classification disallows the existence of ambiguous ratings (such as 3.25), so the expense of misclassification is expected to be bigger than kernel ridge regression. \n",
    "\n",
    "The outperformance of RSVD could be explained by two things:\n",
    "1. When performing matrix factorization, it is the loss function of RSVD that gets optimized, rather than that of KNN or kernel ridge;\n",
    "2. Temporal dynamics are incorporated in the loss function. The binified time components are able to explain some variation in the sample dataset, so it is possible that sometimes a rating is dominated by the bias in time dimension. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
