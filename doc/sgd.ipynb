{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mfsgd(object):\n",
    "    def __init__(self, filename, n=10, penalty=0.5, learning_rate=0.01, stopping_criteria=0.01):\n",
    "        \"\"\"\n",
    "        param learning_rate: minimum 1e-6 \n",
    "        \"\"\"\n",
    "        self.data = mfsgd.preprocess(filename)\n",
    "        self.lr = max(learning_rate, 1e-6)\n",
    "        self.origlr = max(learning_rate, 1e-6)\n",
    "        self.decrement = 1\n",
    "        self.nepoch = 1e6\n",
    "        self.sc = stopping_criteria\n",
    "        self.n = n\n",
    "        self.penalty = penalty\n",
    "        self.train_size = None\n",
    "        self.validation_size = 0\n",
    "        self.n_users = len(self.data.loc[:, 'userId'].unique())\n",
    "        unique_items = self.data.loc[:, 'movieId'].unique()\n",
    "        self.n_items = len(unique_items)\n",
    "        self.item_mapping = dict(zip(unique_items, list(range(len(unique_items)))))\n",
    "        self.time_window = (self.data.loc[:, 'timestamp'].min(), self.data.loc[:, 'timestamp'].max()+1)\n",
    "        \n",
    "    def setLearningRateSchedule(self, start=0.01, decrement=0.1, nepoch=100):\n",
    "        \"\"\"\n",
    "        param start: starting learning rate\n",
    "        param decrement: multiplier to the learning rate per nepoch epochs\n",
    "        param nepoch: number of epochs between two decrements\n",
    "        \"\"\"\n",
    "        self.lr = start\n",
    "        self.origlr = start\n",
    "        self.decrement = decrement\n",
    "        self.nepoch = nepoch\n",
    "        return self\n",
    "    \n",
    "    def fit(self, train_size=0.7, user_nbins=10, item_nbins=3, beta=0.4, n_init=1, n_iter=50):\n",
    "        if train_size > 1:\n",
    "            raise Exception('train_size cannot exceed 1')\n",
    "        #self.r = self.data.groupby('userId').apply(lambda x: x.head(int(len(x)*min(1,train_size)))).reset_index(level=0, drop=True)\n",
    "        self.r = self.data.groupby('userId').apply(lambda x: x.sample(frac=min(1,train_size))).reset_index(level=0, drop=True)\n",
    "        self.train_size = train_size\n",
    "        self.beta = beta\n",
    "        self.user_nbins = user_nbins\n",
    "        self.user_binsize = self.__binify(self.time_window, self.user_nbins)\n",
    "        self.avg_user_bin = {k: self.__timestampToBin(v, self.user_binsize) for k, v in self.r.groupby('userId')['timestamp'].mean().items()}\n",
    "        self.item_nbins = item_nbins\n",
    "        self.item_binsize = self.__binify(self.time_window, self.item_nbins)\n",
    "        self.user_dict = self.r.groupby('userId')['movieId']\n",
    "        self.ru = self.user_dict.count().apply(lambda x:x**(-0.5))\n",
    "        self.train_loss = np.nan\n",
    "        for i in range(n_init):\n",
    "            result = self.__trainEach(n_iter)\n",
    "            if np.isnan(self.train_loss) or result['loss'] < self.train_loss:\n",
    "                self.mu = result['mu']\n",
    "                self.q = result['q']\n",
    "                self.p_user = result['p_user']\n",
    "                self.pa_user = result['pa_user']\n",
    "                self.b_user = result['b_user']\n",
    "                self.a_user = result['a_user']\n",
    "                self.b_item = result['b_item']\n",
    "                self.b_item_bin = result['b_item_bin']\n",
    "                self.y = result['y']\n",
    "                self.train_loss = result['loss']\n",
    "        self.__resetLR()\n",
    "        return self\n",
    "    \n",
    "    def validate(self, validation_size=0.1):\n",
    "        if self.train_size is None:\n",
    "            raise Exception('model is not trained')\n",
    "        if self.train_size == 1:\n",
    "            warnings.warn('train_size = 1, no data can be used to validate')\n",
    "            return\n",
    "        if validation_size + self.train_size > 1:\n",
    "            warnings.warn('validation_size + train_size cannot exceed 1, truncating validation_size to ', 1-self.train_size)\n",
    "            validation_size = 1 - self.train_size\n",
    "        if validation_size == 0:\n",
    "            warnings.warn('validation_size = 0')\n",
    "            return\n",
    "        \n",
    "        self.validation_size = validation_size\n",
    "        pct = self.validation_size / (1 - self.train_size)\n",
    "        self.validation = self.data.drop(self.r.index).groupby('userId').apply(lambda x: x.sample(frac=min(1,pct))).reset_index(level=0, drop=True)\n",
    "        rmse, r_pred = self.__computeLoss(dataset='validation')\n",
    "        print('validation rmse:', rmse)\n",
    "        return r_pred\n",
    "    \n",
    "    def predict(self):\n",
    "        if self.train_size is None:\n",
    "            raise Exception('model is not trained')\n",
    "        if self.train_size + self.validation_size >= 1:\n",
    "            warnings.warn('no data can be used to test')\n",
    "            return\n",
    "        self.test = self.data.drop(self.r.index) if self.validation_size == 0 else self.data.drop(self.r.index.union(self.validation.index))\n",
    "        rmse, r_pred = self.__computeLoss(dataset='test')\n",
    "        print('test rmse:', rmse)\n",
    "        return r_pred\n",
    "        \n",
    "    def __trainEach(self, n_iter):\n",
    "        mu = np.random.uniform(-0.1, 0.1, 1)\n",
    "        q = np.random.uniform(-0.1, 0.1, (self.n, self.n_items))\n",
    "        p_user = np.random.uniform(-0.1, 0.1, (self.n, self.n_users))\n",
    "        pa_user = np.random.uniform(-0.1, 0.1, (self.n, self.n_users))\n",
    "        b_user = np.random.uniform(-0.1, 0.1, self.n_users)\n",
    "        a_user = np.random.uniform(-0.1, 0.1, self.n_users)\n",
    "        b_item = np.random.uniform(-0.1, 0.1, self.n_items)\n",
    "        b_item_bin = np.random.uniform(-0.1, 0.1, (self.item_nbins, self.n_items))\n",
    "        y = np.random.uniform(-0.1, 0.1, (self.n, self.n_items))\n",
    "        \n",
    "        c = 0\n",
    "        for it in range(n_iter):\n",
    "            loss = 0\n",
    "            sTime = time.time()\n",
    "            for ind, s in self.r.iterrows():\n",
    "                u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "                pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "                i_bin = self.__timestampToBin(t, self.item_binsize)\n",
    "                bi, bibin = b_item[i], b_item_bin[i_bin, i]\n",
    "                bu, au = b_user[u], a_user[u]\n",
    "                dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "                ru = self.ru[u+1]\n",
    "                user_items = [self.item_mapping[x] for x in self.user_dict.get_group(u+1)]\n",
    "                yu = np.sum(y[:, user_items], axis=1)\n",
    "                r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev+ru*yu)\n",
    "                res = r - r_hat\n",
    "                # update based on gradient\n",
    "                mu -= self.lr * self.__muDeriv(res)\n",
    "                q[:,i] -= self.lr * self.__qDeriv(res, pu, pua, qi, ru, yu, dev)\n",
    "                p_user[:,u] -= self.lr * self.__puDeriv(res, pu, qi)\n",
    "                pa_user[:, u] -= self.lr * self.__puaDeriv(res, pua, qi, dev)\n",
    "                b_user[u] -= self.lr * self.__buDeriv(res, bu)\n",
    "                a_user[u] -= self.lr *self.__auDeriv(res, au, dev)\n",
    "                b_item[i] -= self.lr * self.__biDeriv(res, bi)\n",
    "                b_item_bin[i_bin, i] -= self.lr * self.__bibinDeriv(res, bibin)\n",
    "                y[:, user_items] -= self.lr * self.__yuDeriv(res, qi, ru, y[:, user_items])\n",
    "                \n",
    "                loss += res**2\n",
    "            # update learning rate\n",
    "            c += 1\n",
    "            if not c%self.nepoch:\n",
    "                self.lr = max(self.lr * self.decrement, 1e-6)\n",
    "            \n",
    "            # use avg residual as loss\n",
    "            loss = np.sqrt(loss / len(self.r))\n",
    "            execTime = time.time() - sTime\n",
    "            \n",
    "            print('epoch', it+1, '----learning rate: {:.6f}'.format(self.lr), '----unpenalized training loss:', loss, \n",
    "                 '----execution time: %s'%execTime)\n",
    "        \n",
    "        return {'loss':loss,\n",
    "                'mu':mu,\n",
    "                'q':q,\n",
    "                'p_user':p_user,\n",
    "                'pa_user':pa_user,\n",
    "                'b_user':b_user,\n",
    "                'a_user':a_user,\n",
    "                'b_item':b_item,\n",
    "                'b_item_bin':b_item_bin,\n",
    "                'y':y}\n",
    "        \n",
    "    def __computeLoss(self, dataset='train', **kwargs):\n",
    "        loss = 0\n",
    "        r_pred = None\n",
    "        if dataset == 'train':\n",
    "            data = self.r\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin, y = kwargs['mu'], kwargs['q'], kwargs['p_user'], kwargs['pa_user'], kwargs['b_user'], kwargs['a_user'], kwargs['b_item'], kwargs['b_item_bin'], kwargs['y']\n",
    "        elif dataset in ['test', 'validation']:\n",
    "            data = self.test if dataset == 'test' else self.validation\n",
    "            r_pred = np.zeros(len(data))\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin, y = self.mu, self.q, self.p_user, self.pa_user, self.b_user, self.a_user, self.b_item, self.b_item_bin, self.y\n",
    "        else:\n",
    "            raise Exception('ambiguous compute loss inputs')\n",
    "        \n",
    "        for ind, s in data.reset_index().iterrows():\n",
    "            u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "            pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "            bi, bibin = b_item[i], b_item_bin[self.__timestampToBin(t, self.item_binsize), i]\n",
    "            bu, au = b_user[u], a_user[u]\n",
    "            dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "            ru = self.ru[u+1]\n",
    "            user_items = [self.item_mapping[x] for x in self.user_dict.get_group(u+1)]\n",
    "            yu = np.sum(y[:, user_items], axis=1)\n",
    "            r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev+ru*yu)\n",
    "            res = (r-r_hat)**2\n",
    "            if dataset == 'train':\n",
    "                loss += res + self.penalty*(bi**2+bibin**2+bu**2+au**2+npla.norm(pu)**2+npla.norm(pua)**2+npla.norm(qi)**2)\n",
    "            else:\n",
    "                loss += res\n",
    "                r_pred[ind] = r_hat\n",
    "\n",
    "        return np.sqrt(loss / len(data)), r_pred\n",
    "    \n",
    "    # FIXME, update qDeriv\n",
    "    def __muDeriv(self, res):\n",
    "        return -res\n",
    "    \n",
    "    def __qDeriv(self, res, pu, pua, qi, ru, yu, dev):\n",
    "        return -res * (pu+pua*dev+ru*yu) + self.penalty * qi\n",
    "    \n",
    "    def __puDeriv(self, res, pu, qi):\n",
    "        return -res * qi + self.penalty * pu\n",
    "    \n",
    "    def __puaDeriv(self, res, pua, qi, dev):\n",
    "        return -res * qi * dev + self.penalty * pua\n",
    "    \n",
    "    def __buDeriv(self, res, bu):\n",
    "        return -res + self.penalty * bu\n",
    "    \n",
    "    def __auDeriv(self, res, au, dev):\n",
    "        return -res * dev + self.penalty * au\n",
    "    \n",
    "    def __biDeriv(self, res, bi):\n",
    "        return -res + self.penalty * bi\n",
    "    \n",
    "    def __bibinDeriv(self, res, bibin):\n",
    "        return -res + self.penalty * bibin\n",
    "    \n",
    "    def __yuDeriv(self, res, qi, ru, yu):\n",
    "        return -res * qi[:, np.newaxis] * ru + self.penalty * yu\n",
    "    \n",
    "    # FIXME, add y and R(u)^(-1/2)\n",
    "    def __dev(self, t, avg, b):\n",
    "        return np.sign(t-avg) * np.abs(t-avg)**b\n",
    "    \n",
    "    def __binify(self, window, nbins):\n",
    "        return (window[1] - window[0]) / nbins\n",
    "    \n",
    "    def __timestampToBin(self, t, binsize):\n",
    "        if t < self.time_window[0] or t > self.time_window[1]:\n",
    "            raise Exception('t outside of time window')\n",
    "        return int((t - self.time_window[0]) // binsize)\n",
    "    \n",
    "    def __resetLR(self):\n",
    "        self.lr = self.origlr\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('G:\\mawenwen\\Columbia\\Fall 2019\\Applied Data Science\\proj4','fall2019-project4-sec1-grp4-master\\data\\ml-latest-small','ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x2283f606630>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mfsgd(filename=f, n=30, penalty=0.1) # learning rate should not be > 0.1 as it results in overflow in loss calculation\n",
    "s.setLearningRateSchedule(start=0.05, decrement=0.2, nepoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [0.91745402] ----execution time: 64.10300946235657\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.83057557] ----execution time: 62.055584192276\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.76573024] ----execution time: 61.90699863433838\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.70996316] ----execution time: 63.165032148361206\n",
      "epoch 5 ----learning rate: 0.010000 ----unpenalized training loss: [0.66422816] ----execution time: 62.12339997291565\n",
      "epoch 6 ----learning rate: 0.010000 ----unpenalized training loss: [0.62456904] ----execution time: 61.79437327384949\n",
      "epoch 7 ----learning rate: 0.010000 ----unpenalized training loss: [0.61271088] ----execution time: 72.03207802772522\n",
      "epoch 8 ----learning rate: 0.010000 ----unpenalized training loss: [0.60558325] ----execution time: 69.40960621833801\n",
      "epoch 9 ----learning rate: 0.010000 ----unpenalized training loss: [0.59984269] ----execution time: 71.27805590629578\n",
      "epoch 10 ----learning rate: 0.002000 ----unpenalized training loss: [0.5947534] ----execution time: 69.54243421554565\n",
      "epoch 11 ----learning rate: 0.002000 ----unpenalized training loss: [0.59732727] ----execution time: 69.65315651893616\n",
      "epoch 12 ----learning rate: 0.002000 ----unpenalized training loss: [0.59354753] ----execution time: 73.73587417602539\n",
      "epoch 13 ----learning rate: 0.002000 ----unpenalized training loss: [0.59134212] ----execution time: 73.64251446723938\n",
      "epoch 14 ----learning rate: 0.002000 ----unpenalized training loss: [0.58968197] ----execution time: 74.4906017780304\n",
      "epoch 15 ----learning rate: 0.000400 ----unpenalized training loss: [0.58828743] ----execution time: 75.63452792167664\n",
      "epoch 16 ----learning rate: 0.000400 ----unpenalized training loss: [0.5921221] ----execution time: 70.96419286727905\n",
      "epoch 17 ----learning rate: 0.000400 ----unpenalized training loss: [0.59091629] ----execution time: 69.33171534538269\n",
      "epoch 18 ----learning rate: 0.000400 ----unpenalized training loss: [0.58992419] ----execution time: 71.24325013160706\n",
      "epoch 19 ----learning rate: 0.000400 ----unpenalized training loss: [0.58913618] ----execution time: 65.01699471473694\n",
      "epoch 20 ----learning rate: 0.000080 ----unpenalized training loss: [0.58847776] ----execution time: 62.0382981300354\n",
      "epoch 21 ----learning rate: 0.000080 ----unpenalized training loss: [0.59066547] ----execution time: 61.420809745788574\n",
      "epoch 22 ----learning rate: 0.000080 ----unpenalized training loss: [0.58989222] ----execution time: 62.287962913513184\n",
      "epoch 23 ----learning rate: 0.000080 ----unpenalized training loss: [0.58963676] ----execution time: 64.37522149085999\n",
      "epoch 24 ----learning rate: 0.000080 ----unpenalized training loss: [0.58939804] ----execution time: 71.06255912780762\n",
      "epoch 25 ----learning rate: 0.000016 ----unpenalized training loss: [0.58917384] ----execution time: 72.62740468978882\n",
      "epoch 26 ----learning rate: 0.000016 ----unpenalized training loss: [0.58948065] ----execution time: 68.01605582237244\n",
      "epoch 27 ----learning rate: 0.000016 ----unpenalized training loss: [0.58931003] ----execution time: 71.31916904449463\n",
      "epoch 28 ----learning rate: 0.000016 ----unpenalized training loss: [0.58924191] ----execution time: 68.8313660621643\n",
      "epoch 29 ----learning rate: 0.000016 ----unpenalized training loss: [0.58919] ----execution time: 61.60219645500183\n",
      "epoch 30 ----learning rate: 0.000003 ----unpenalized training loss: [0.58914203] ----execution time: 62.811466693878174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x2283f606630>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.fit(train_size=0.8, user_nbins=10, item_nbins=3, beta=0.6, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation rmse: [0.86355463]\n"
     ]
    }
   ],
   "source": [
    "r_validate = s.validate(validation_size=0.1) # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse: [0.85878478]\n"
     ]
    }
   ],
   "source": [
    "r_test = s.predict() # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
