{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mfsgd(object):\n",
    "    def __init__(self, filename, n=10, penalty=0.5, learning_rate=0.01, stopping_criteria=0.01):\n",
    "        \"\"\"\n",
    "        param learning_rate: minimum 1e-6 \n",
    "        \"\"\"\n",
    "        self.data = mfsgd.preprocess(filename)\n",
    "        self.lr = max(learning_rate, 1e-6)\n",
    "        self.origlr = max(learning_rate, 1e-6)\n",
    "        self.decrement = 1\n",
    "        self.nepoch = 1e6\n",
    "        self.sc = stopping_criteria\n",
    "        self.n = n\n",
    "        self.penalty = penalty\n",
    "        self.train_size = None\n",
    "        self.validation_size = 0\n",
    "        self.n_users = len(self.data.loc[:, 'userId'].unique())\n",
    "        unique_items = self.data.loc[:, 'movieId'].unique()\n",
    "        self.n_items = len(unique_items)\n",
    "        self.item_mapping = dict(zip(unique_items, list(range(len(unique_items)))))\n",
    "        \n",
    "    def setLearningRateSchedule(self, start=0.01, decrement=0.1, nepoch=100):\n",
    "        \"\"\"\n",
    "        param start: starting learning rate\n",
    "        param decrement: multiplier to the learning rate per nepoch epochs\n",
    "        param nepoch: number of epochs between two decrements\n",
    "        \"\"\"\n",
    "        self.lr = start\n",
    "        self.origlr = start\n",
    "        self.decrement = decrement\n",
    "        self.nepoch = nepoch\n",
    "        return self\n",
    "    \n",
    "    def fit(self, train_size=0.7, user_nbins=10, item_nbins=3, beta=0.4, n_init=1, n_iter=50):\n",
    "        if train_size > 1:\n",
    "            raise Exception('train_size cannot exceed 1')\n",
    "        self.r = self.data.groupby('userId').apply(lambda x: x.head(int(len(x)*min(1,train_size)))).reset_index(level=0, drop=True)\n",
    "        self.train_size = train_size\n",
    "        self.beta = beta\n",
    "        self.time_window = (self.r.loc[:, 'timestamp'].min(), self.r.loc[:, 'timestamp'].max()+1)\n",
    "        # FIXME, user_nbins can be a dictionary for each user\n",
    "        self.user_nbins = user_nbins\n",
    "        self.user_binsize = self.__binify(self.time_window, self.user_nbins)\n",
    "        self.avg_user_bin = {k: self.__timestampToBin(v, self.user_binsize) for k, v in self.r.groupby('userId')['timestamp'].mean().items()}\n",
    "        self.item_nbins = item_nbins\n",
    "        self.item_binsize = self.__binify(self.time_window, self.item_nbins)\n",
    "        #self.ru = self.r.groupby('userId').count()\n",
    "        \n",
    "        self.train_loss = np.nan\n",
    "        for i in range(n_init):\n",
    "            result = self.__trainEach(n_iter)\n",
    "            if np.isnan(self.train_loss) or result['loss'] < self.train_loss:\n",
    "                self.mu = result['mu']\n",
    "                self.q = result['q']\n",
    "                self.p_user = result['p_user']\n",
    "                self.pa_user = result['pa_user']\n",
    "                self.b_user = result['b_user']\n",
    "                self.a_user = result['a_user']\n",
    "                self.b_item = result['b_item']\n",
    "                self.b_item_bin = result['b_item_bin']\n",
    "                self.train_loss = result['loss']\n",
    "        self.__resetLR()\n",
    "        return self\n",
    "    \n",
    "    def validate(self, validation_size=0.1):\n",
    "        if self.train_size is None:\n",
    "            raise Exception('model is not trained')\n",
    "        if self.train_size == 1:\n",
    "            warnings.warn('train_size = 1, no data can be used to validate')\n",
    "            return\n",
    "        if validation_size + self.train_size > 1:\n",
    "            warnings.warn('validation_size + train_size cannot exceed 1, truncating validation_size to ', 1-self.train_size)\n",
    "            validation_size = 1 - self.train_size\n",
    "        if validation_size == 0:\n",
    "            warnings.warn('validation_size = 0')\n",
    "            return\n",
    "        \n",
    "        self.validation_size = validation_size\n",
    "        pct = self.validation_size / (1 - self.train_size)\n",
    "        self.validation = self.data.drop(self.r.index).groupby('userId').apply(lambda x: x.head(int(len(x)*min(1,pct)))).reset_index(level=0, drop=True)\n",
    "        rmse, r_pred = self.__computeLoss(dataset='validation')\n",
    "        print('validation rmse:', rmse)\n",
    "        return r_pred\n",
    "    \n",
    "    def predict(self):\n",
    "        if self.train_size is None:\n",
    "            raise Exception('model is not trained')\n",
    "        if self.train_size + self.validation_size >= 1:\n",
    "            warnings.warn('no data can be used to test')\n",
    "            return\n",
    "        self.test = self.data.drop(self.r.index) if self.validation_size == 0 else self.data.drop(self.r.index.union(self.validation.index))\n",
    "        rmse, r_pred = self.__computeLoss(dataset='test')\n",
    "        print('test rmse:', rmse)\n",
    "        return r_pred\n",
    "        \n",
    "    def __trainEach(self, n_iter):\n",
    "        mu = np.random.uniform(-1, 1, 1)\n",
    "        q = np.random.uniform(-1, 1, (self.n, self.n_items))\n",
    "        p_user = np.random.uniform(-1, 1, (self.n, self.n_users))\n",
    "        pa_user = np.random.uniform(-1, 1, (self.n, self.n_users))\n",
    "        b_user = np.random.uniform(-1, 1, self.n_users)\n",
    "        a_user = np.random.uniform(-1, 1, self.n_users)\n",
    "        b_item = np.random.uniform(-1, 1, self.n_items)\n",
    "        b_item_bin = np.random.uniform(-1, 1, (self.item_nbins, self.n_items))\n",
    "        \n",
    "        # FIXME, add Ru**(-1/2), y\n",
    "        \n",
    "        c = 0\n",
    "        for it in range(n_iter):\n",
    "            loss = 0\n",
    "            for ind, s in self.r.iterrows():\n",
    "                u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "                pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "                i_bin = self.__timestampToBin(t, self.item_binsize)\n",
    "                bi, bibin = b_item[i], b_item_bin[i_bin, i]\n",
    "                bu, au = b_user[u], a_user[u]\n",
    "                dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "                r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev)\n",
    "                res = r - r_hat\n",
    "                # update based on gradient\n",
    "                mu -= self.lr * self.__muDeriv(res)\n",
    "                q[:,i] -= self.lr * self.__qDeriv(res, pu, pua, qi, dev)\n",
    "                p_user[:,u] -= self.lr * self.__puDeriv(res, pu, qi)\n",
    "                pa_user[:, u] -= self.lr * self.__puaDeriv(res, pua, qi, dev)\n",
    "                b_user[u] -= self.lr * self.__buDeriv(res, bu)\n",
    "                a_user[u] -= self.lr *self.__auDeriv(res, au, dev)\n",
    "                b_item[i] -= self.lr * self.__biDeriv(res, bi)\n",
    "                b_item_bin[i_bin, i] -= self.lr * self.__bibinDeriv(res, bibin)\n",
    "                \n",
    "                loss += res**2\n",
    "            # update learning rate\n",
    "            c += 1\n",
    "            if not c%self.nepoch:\n",
    "                self.lr = max(self.lr * self.decrement, 1e-6)\n",
    "            \n",
    "            # use avg residual as loss\n",
    "            loss = np.sqrt(loss / len(self.r))\n",
    "            # dont compute loss again as complexity is high\n",
    "            # loss, _ = self.__computeLoss(mu=mu, q=q, p_user=p_user, pa_user=pa_user, \n",
    "            #                              b_user=b_user, a_user=a_user, b_item=b_item, b_item_bin=b_item_bin)\n",
    "            \n",
    "            print('epoch', it+1, '----learning rate: {:.6f}'.format(self.lr), '----unpenalized training loss:', loss)\n",
    "        \n",
    "        return {'loss':loss,\n",
    "                'mu':mu,\n",
    "                'q':q,\n",
    "                'p_user':p_user,\n",
    "                'pa_user':pa_user,\n",
    "                'b_user':b_user,\n",
    "                'a_user':a_user,\n",
    "                'b_item':b_item,\n",
    "                'b_item_bin':b_item_bin}\n",
    "        \n",
    "    def __computeLoss(self, dataset='train', **kwargs):\n",
    "        loss = 0\n",
    "        r_pred = None\n",
    "        if dataset == 'train':\n",
    "            data = self.r\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin = kwargs['mu'], kwargs['q'], kwargs['p_user'], kwargs['pa_user'], kwargs['b_user'], kwargs['a_user'], kwargs['b_item'], kwargs['b_item_bin']\n",
    "        elif dataset in ['test', 'validation']:\n",
    "            data = self.test if dataset == 'test' else self.validation\n",
    "            r_pred = np.zeros(len(data))\n",
    "            mu, q, p_user, pa_user, b_user, a_user, b_item, b_item_bin = self.mu, self.q, self.p_user, self.pa_user, self.b_user, self.a_user, self.b_item, self.b_item_bin\n",
    "        else:\n",
    "            raise Exception('ambiguous compute loss inputs')\n",
    "        \n",
    "        for ind, s in data.reset_index().iterrows():\n",
    "            u, i, r, t = int(s['userId'])-1, self.item_mapping[int(s['movieId'])], s['rating'], s['timestamp']\n",
    "            pu, pua, qi = p_user[:, u], pa_user[:, u], q[:, i]\n",
    "            bi, bibin = b_item[i], b_item_bin[self.__timestampToBin(t, self.item_binsize), i]\n",
    "            bu, au = b_user[u], a_user[u]\n",
    "            dev = self.__dev(self.__timestampToBin(t, self.user_binsize), self.avg_user_bin[u+1], self.beta)\n",
    "            r_hat = mu+bi+bibin+bu+au*dev+qi@(pu+pua*dev)\n",
    "            res = (r-r_hat)**2\n",
    "            if dataset == 'train':\n",
    "                loss += res + self.penalty*(bi**2+bibin**2+bu**2+au**2+npla.norm(pu)**2+npla.norm(pua)**2+npla.norm(qi)**2)\n",
    "            else:\n",
    "                loss += res\n",
    "                r_pred[ind] = r_hat\n",
    "\n",
    "        return np.sqrt(loss / len(data)), r_pred\n",
    "    \n",
    "    # FIXME, update qDeriv\n",
    "    def __muDeriv(self, res):\n",
    "        return -res\n",
    "    \n",
    "    def __qDeriv(self, res, pu, pua, qi, dev):\n",
    "        return -res * (pu+pua*dev) + self.penalty * qi\n",
    "    \n",
    "    def __puDeriv(self, res, pu, qi):\n",
    "        return -res * qi + self.penalty * pu\n",
    "    \n",
    "    def __puaDeriv(self, res, pua, qi, dev):\n",
    "        return -res * qi * dev + self.penalty * pua\n",
    "    \n",
    "    def __buDeriv(self, res, bu):\n",
    "        return -res + self.penalty * bu\n",
    "    \n",
    "    def __auDeriv(self, res, au, dev):\n",
    "        return -res * dev + self.penalty * au\n",
    "    \n",
    "    def __biDeriv(self, res, bi):\n",
    "        return -res + self.penalty * bi\n",
    "    \n",
    "    def __bibinDeriv(self, res, bibin):\n",
    "        return -res + self.penalty * bibin\n",
    "    \n",
    "    # FIXME, add y and R(u)^(-1/2)\n",
    "    def __dev(self, t, avg, b):\n",
    "        return np.sign(t-avg) * np.abs(t-avg)**b\n",
    "    \n",
    "    def __binify(self, window, nbins):\n",
    "        return (window[1] - window[0]) / nbins\n",
    "    \n",
    "    def __timestampToBin(self, t, binsize):\n",
    "        return int((t - self.time_window[0]) // binsize)\n",
    "    \n",
    "    def __resetLR(self):\n",
    "        self.lr = self.origlr\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(filename):\n",
    "        data = pd.read_csv(filename)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('G:\\mawenwen\\Columbia\\Fall 2019\\Applied Data Science\\proj4','fall2019-project4-sec1-grp4-master\\data\\ml-latest-small','ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x24d13e02358>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mfsgd(filename=f, n=20, penalty=1.5) # learning rate should not be > 0.1 as it results in overflow in loss calculation\n",
    "s.setLearningRateSchedule(start=0.05, decrement=0.1, nepoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ----learning rate: 0.050000 ----unpenalized training loss: [1.01370978]\n",
      "epoch 2 ----learning rate: 0.050000 ----unpenalized training loss: [0.89587431]\n",
      "epoch 3 ----learning rate: 0.050000 ----unpenalized training loss: [0.87254245]\n",
      "epoch 4 ----learning rate: 0.050000 ----unpenalized training loss: [0.86095344]\n",
      "epoch 5 ----learning rate: 0.050000 ----unpenalized training loss: [0.85403175]\n",
      "epoch 6 ----learning rate: 0.050000 ----unpenalized training loss: [0.84950416]\n",
      "epoch 7 ----learning rate: 0.050000 ----unpenalized training loss: [0.8463757]\n",
      "epoch 8 ----learning rate: 0.050000 ----unpenalized training loss: [0.84413241]\n",
      "epoch 9 ----learning rate: 0.050000 ----unpenalized training loss: [0.84247949]\n",
      "epoch 10 ----learning rate: 0.050000 ----unpenalized training loss: [0.84123547]\n",
      "epoch 11 ----learning rate: 0.050000 ----unpenalized training loss: [0.84028288]\n",
      "epoch 12 ----learning rate: 0.005000 ----unpenalized training loss: [0.83954274]\n",
      "epoch 13 ----learning rate: 0.005000 ----unpenalized training loss: [0.84672342]\n",
      "epoch 14 ----learning rate: 0.005000 ----unpenalized training loss: [0.83507247]\n",
      "epoch 15 ----learning rate: 0.005000 ----unpenalized training loss: [0.83159127]\n",
      "epoch 16 ----learning rate: 0.005000 ----unpenalized training loss: [0.82998575]\n",
      "epoch 17 ----learning rate: 0.005000 ----unpenalized training loss: [0.8290579]\n",
      "epoch 18 ----learning rate: 0.005000 ----unpenalized training loss: [0.82844617]\n",
      "epoch 19 ----learning rate: 0.005000 ----unpenalized training loss: [0.82800742]\n",
      "epoch 20 ----learning rate: 0.005000 ----unpenalized training loss: [0.82767372]\n",
      "epoch 21 ----learning rate: 0.005000 ----unpenalized training loss: [0.82740851]\n",
      "epoch 22 ----learning rate: 0.005000 ----unpenalized training loss: [0.82719037]\n",
      "epoch 23 ----learning rate: 0.005000 ----unpenalized training loss: [0.82700592]\n",
      "epoch 24 ----learning rate: 0.000500 ----unpenalized training loss: [0.82684639]\n",
      "epoch 25 ----learning rate: 0.000500 ----unpenalized training loss: [0.85243416]\n",
      "epoch 26 ----learning rate: 0.000500 ----unpenalized training loss: [0.84516838]\n",
      "epoch 27 ----learning rate: 0.000500 ----unpenalized training loss: [0.8419251]\n",
      "epoch 28 ----learning rate: 0.000500 ----unpenalized training loss: [0.84006476]\n",
      "epoch 29 ----learning rate: 0.000500 ----unpenalized training loss: [0.83886196]\n",
      "epoch 30 ----learning rate: 0.000500 ----unpenalized training loss: [0.83802376]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.mfsgd at 0x24d13e02358>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.fit(train_size=0.8, user_nbins=8, item_nbins=3, beta=0.3, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation rmse: [0.98889374]\n"
     ]
    }
   ],
   "source": [
    "r_validate = s.validate(validation_size=0.1) # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse: [1.05286189]\n"
     ]
    }
   ],
   "source": [
    "r_test = s.predict() # return predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
